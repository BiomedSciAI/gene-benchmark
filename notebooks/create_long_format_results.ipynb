{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from yaml import safe_load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores(df_cols):\n",
    "    return list(filter(lambda x: \"test_\" in x, df_cols.columns))\n",
    "\n",
    "\n",
    "def load_yaml_file(model_config):\n",
    "    with open(model_config) as f:\n",
    "        loaded_yaml = safe_load(f)\n",
    "    return loaded_yaml\n",
    "\n",
    "\n",
    "def to_long_form(df_report, score_list, fold_num=5, score=\"score\", derived=False):\n",
    "    repeated_columns = [\"task_name\", \"task_group\", \"model_name\"]\n",
    "    if derived:\n",
    "        repeated_columns.append(\"sub_task\")\n",
    "    cols = repeated_columns + [\n",
    "        \"fold\",\n",
    "        \"metric\",\n",
    "        score,\n",
    "    ]\n",
    "    long_df = pd.DataFrame(\n",
    "        index=range(df_report.shape[0] * fold_num * len(score_list)), columns=cols\n",
    "    )\n",
    "    idx = 0\n",
    "    for _, df_row in df_report.iterrows():\n",
    "        for curr_score in score_list:\n",
    "            for fold, value in enumerate(df_row[curr_score].split(\",\")):\n",
    "                long_df.iloc[idx, :] = list(df_row[repeated_columns]) + [\n",
    "                    fold,\n",
    "                    curr_score,\n",
    "                    float(value),\n",
    "                ]\n",
    "                idx = idx + 1\n",
    "    return long_df\n",
    "\n",
    "\n",
    "def get_log_from_df(file_name, family_dict, task_type, derived=False):\n",
    "    res_df = pd.read_csv(file_name)\n",
    "    res_df[\"task_group\"] = res_df[\"task_name\"].apply(\n",
    "        lambda x: None if x not in family_dict else family_dict[x]\n",
    "    )\n",
    "\n",
    "    res_df = to_long_form(res_df, get_scores(res_df), derived=derived)\n",
    "    res_df[\"task_type\"] = task_type\n",
    "    res_df[\"score\"] = res_df[\"score\"].astype(float)\n",
    "    res_df[\"metric\"] = res_df[\"metric\"].apply(lambda x: x.replace(\"test_\", \"\"))\n",
    "    res_df[\"metric\"] = res_df[\"metric\"].apply(lambda x: x.replace(\"_weighted\", \"\"))\n",
    "    res_df[\"metric\"] = res_df[\"metric\"].apply(lambda x: x.replace(\"_ovr\", \"\"))\n",
    "    return res_df\n",
    "\n",
    "\n",
    "def mean_std_string(vals):\n",
    "    mean_val = np.mean(vals)\n",
    "    if np.isnan(mean_val):\n",
    "        return \"-\"\n",
    "    elif mean_val > 1:\n",
    "        f\"{np.mean(vals):.0f} ({np.std(vals):.0f})\"\n",
    "    else:\n",
    "        return f\"{np.mean(vals):.2f} ({np.std(vals):.2f})\"\n",
    "\n",
    "\n",
    "model_type = {\n",
    "    \"mpnet\": \"LLM\",\n",
    "    \"bag_of_words\": \"Classical ML\",\n",
    "    \"GEARS\": \"ScRNA-seq\",\n",
    "    \"ScGPT\": \"ScRNA-seq\",\n",
    "    \"top_mteb\": \"LLM\",\n",
    "    \"geneformer\": \"ScRNA-seq\",\n",
    "    \"cellPLM\": \"ScRNA-seq\",\n",
    "    \"cellPT\": \"ScRNA-seq\",\n",
    "    \"gene2vec\": \"Classical ML\",\n",
    "    \"mistral\": \"LLM\",\n",
    "}\n",
    "model_name = {\n",
    "    \"mpnet\": \"MPNet\",\n",
    "    \"bag_of_words\": \"Bag of Words\",\n",
    "    \"GEARS\": \"GEARS\",\n",
    "    \"ScGPT\": \"ScGPT\",\n",
    "    \"top_mteb\": \"MTEB-1B\",\n",
    "    \"geneformer\": \"Geneformer\",\n",
    "    \"cellPLM\": \"cellPLM\",\n",
    "    \"cellPT\": \"cellPLM\",\n",
    "    \"gene2vec\": \"Gene2vec\",\n",
    "    \"mistral\": \"MTEB-7B\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = Path(\"/dccstor/bmfm-targets/text_models/manuscript\")\n",
    "binary_report = base_folder / \"performance/binary_tasks.csv\"\n",
    "multi_label_report = base_folder / \"performance/multi_label_tasks.csv\"\n",
    "regression_report = base_folder / \"performance/regression_tasks.csv\"\n",
    "category_report = base_folder / \"performance/categorical_tasks.csv\"\n",
    "derived_report = base_folder / \"performance/binary_tasks_from_multi_label.csv\"\n",
    "task_family = base_folder / \"task_family_dict.yaml\"\n",
    "\n",
    "task_group = load_yaml_file(task_family)\n",
    "binary_long = get_log_from_df(binary_report, task_group, \"binary\")\n",
    "regression_long = get_log_from_df(regression_report, task_group, \"regression\")\n",
    "category_long = get_log_from_df(category_report, task_group, \"category\")\n",
    "multi_long = get_log_from_df(multi_label_report, task_group, \"multi label\")\n",
    "derived_long = get_log_from_df(derived_report, task_group, \"derived bin\", derived=True)\n",
    "\n",
    "\n",
    "multi_long[\"metric\"] = multi_long[\"metric\"].apply(\n",
    "    lambda x: x if x != \"auc\" else \"roc_auc\"\n",
    ")\n",
    "multi_long[\"metric\"] = multi_long[\"metric\"].apply(\n",
    "    lambda x: x if x != \"hamming_loss\" else \"hamming\"\n",
    ")\n",
    "bin_cat_df = pd.concat([binary_long, category_long, multi_long])\n",
    "\n",
    "bin_cat_df[\"Model family\"] = bin_cat_df[\"model_name\"].apply(lambda x: model_type[x])\n",
    "bin_cat_df[\"Model\"] = bin_cat_df[\"model_name\"].apply(lambda x: model_name[x])\n",
    "\n",
    "\n",
    "derived_long[\"Model family\"] = derived_long[\"model_name\"].apply(lambda x: model_type[x])\n",
    "derived_long[\"Model\"] = derived_long[\"model_name\"].apply(lambda x: model_name[x])\n",
    "\n",
    "bin_cat_df.to_csv(\n",
    "    \"/dccstor/bmfm-targets/text_models/manuscript/performance/results_binary_categorical_multi_long_format.csv\",\n",
    "    index=False,\n",
    ")\n",
    "derived_long.to_csv(\n",
    "    \"/dccstor/bmfm-targets/text_models/manuscript/performance/results_derived_binary_long_format.csv\",\n",
    "    index=False,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
