{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating and benchmarking gene representations using Granite\n",
    "The third generation of [Granite AI language models](https://www.ibm.com/granite) can deliver exceptional performance across a wide range of enterprise tasks, and Gene-Benchmarks is a package to benchmark pre-trained models on downstream gene-related tasks. In this notebook we will use the Granite models to create gene embeddings and then use the Gene-Benchmark package by doing do the following:\n",
    "1. Extract gene embeddings from the granite model\n",
    "2. Evaluate the quality of the embedding on a prediction task.\n",
    "3. Two special bonuses for the dedicated reader who goes through the entire notebook\n",
    "\n",
    "Together, this will give us a comprehensive picture of how well the Granite model can “understand” the properties of genes (more background can be found in our manuscript - [Does your model understand genes? A benchmark of gene properties for biological and text models\n",
    "](https://arxiv.org/abs/2412.04075)).   \n",
    "Let’s start by setting up the environment: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "\n",
    "from gene_benchmark.descriptor import NCBIDescriptor\n",
    "from gene_benchmark.encoder import SentenceTransformerEncoder\n",
    "from gene_benchmark.tasks import EntitiesTask, load_task_definition\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is a task in gene-benchmark\n",
    "\n",
    "The gene-benchmark package includes 293 gene-related tasks testing the ability to predict various properties. Each task is defined by a set of entities (gene symbols) and their corresponding properties (outcomes). For this notebook we use an example of a task called \"bivalent vs non-methylated\", which includes a set of genes and an indication of whether they are bivalent or non-methylated (see further details in this [manuscript](https://arxiv.org/pdf/2412.04075)).  \n",
    "\n",
    "Here’s a quick example of how to look at the definitions of the task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>symbol</th>\n",
       "      <th>Outcomes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HOXA11</td>\n",
       "      <td>bivalent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PAX6</td>\n",
       "      <td>bivalent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ZIC2</td>\n",
       "      <td>bivalent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>COASY</td>\n",
       "      <td>bivalent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PAX2</td>\n",
       "      <td>bivalent</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   symbol  Outcomes\n",
       "0  HOXA11  bivalent\n",
       "1    PAX6  bivalent\n",
       "2    ZIC2  bivalent\n",
       "3   COASY  bivalent\n",
       "4    PAX2  bivalent"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task = load_task_definition(\"bivalent vs non-methylated\",tasks_folder=\"../tasks\")\n",
    "pd.concat([task.entities,task.outcomes],axis=1).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Extract gene embeddings from the granite model\n",
    "The question of gene embeddings from a text model is complex. We could potentially just give the model the name of the gene and extract the embeddings. However, in our experience this provides very poor performance since the name is usually some arbitrary abbreviation of the gene description (for example, the gene BRCA1 is shorthand for “Breast Cancer Type 1 Susceptibility Protein”).  \n",
    "\n",
    "We therefore opted to provide the text models with a more comprehensive description of the gene.The Gene-Benchmark package provides easy functions to extract the gene description from the [NCBI](https://www.ncbi.nlm.nih.gov/gene/), resulting in a data frame with the required descriptions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/opt/miniconda3/envs/geneb/lib/python3.10/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 0",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m description_builder\u001b[38;5;241m=\u001b[39mNCBIDescriptor()\n\u001b[1;32m      2\u001b[0m descriptions \u001b[38;5;241m=\u001b[39m description_builder\u001b[38;5;241m.\u001b[39mdescribe(task\u001b[38;5;241m.\u001b[39mentities)\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdescriptions\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/geneb/lib/python3.10/site-packages/pandas/core/frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m/opt/miniconda3/envs/geneb/lib/python3.10/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "description_builder=NCBIDescriptor()\n",
    "descriptions = description_builder.describe(task.entities)\n",
    "print(descriptions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The package also provides a wrapper function that gives the gene description as input to the language model and then extract the embeddings.  \n",
    "Any text model that following the HuggingFace [SentenceTransformer](https://sbert.net/) API can be used, here we will use the [granite embedding model](https://huggingface.co/ibm-granite/granite-embedding-125m-english) that delivers high-performance sentence-transformer models optimized for retrieval, generating precise embeddings and is built on ethically sourced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "granite_model = \"ibm-granite/granite-embedding-125m-english\"\n",
    "granite_embedding = SentenceTransformerEncoder(granite_model)\n",
    "encodings = granite_embedding.encode(descriptions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Evaluate the quality of the embedding on a prediction task.\n",
    "Now that we have the embeddings, we can estimate their utility for the task. Gene-benchmark provide k-fold cross validation of the prediction task to evaluate the performance and standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = EntitiesTask(\n",
    "    task=\"bivalent vs non-methylated\",\n",
    "    description_builder=NCBIDescriptor(),\n",
    "    encoder=SentenceTransformerEncoder(granite_model),\n",
    "    tasks_folder=\"../tasks\"\n",
    ")\n",
    "_ = task.run()\n",
    "print(f\" mean AUC: {task.summary()['mean_roc_auc']:.2f} sd: {task.summary()['sd_roc_auc']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: evaluating multiple encodings and multiple tasks\n",
    "We showed an example of how to create the embeddings for a specific task. Batch evaluation of multiple tasks and models it can be done using the `run_task.py` script, which outputs the performance into a CSV file.  \n",
    "This requires defining a YAML file for each model, defining the name of the model, the task list, etc. For example, the three YAML files in [this folder](./granite_files) allow running evaluations of 3 versions of granite on two tasks available in Gene-Benchmarks using the following command line:\n",
    "\n",
    "`python run_task.py -t \"Gene2Gene\" -t \"long vs short range TF\"  -m granite_files/granite8b.yaml -m granite_files/granite2b.yaml -m granite_files/graniteEmbed.yaml --output-file-name /performance/bin.csv`  \n",
    "\n",
    "## Bonus 2: evaluating multiple encodings and multiple tasks using lists\n",
    "\n",
    "If you want to run the models on a list of tasks you can use the task list file in [this folder](../scripts/task_configs) and run the script like this:  \n",
    "\n",
    "`python run_task.py -t   -m granite_files/granite8b.yaml -m granite_files/granite2b.yaml -m granite_files/graniteEmbed.yaml --output-file-name /performance/bin.csv -t scripts/task_configs/binary_tasks.yaml `  \n",
    "\n",
    "To run the script on multi-label or multi-class labels the user need to set the right scoring using the  scoring parameter `-s` see the script for additional details. \n",
    "We used the script to compare the performance on 293 tasks of three models - [Granite8B](https://huggingface.co/ibm-granite/granite-3.1-8b-base), [Granite2B](https://huggingface.co/ibm-granite/granite-3.1-2b-base) and [Granite embedding model](https://huggingface.co/ibm-granite/granite-embedding-125m-english).  \n",
    "\n",
    "  \n",
    "The overall results are presented as a heatmap. we can see that the embeddings model might be very small but it is comparable in performance to the larger general LLM models.  \n",
    "\n",
    "![overall](./granite_files/overall.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can have a more finegrained understanding by looking at each task:  \n",
    "\n",
    "![performance](./granite_files/long.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For additional details see our manuscript [Does your model understand genes? A benchmark of gene properties for biological and text models\n",
    "](https://arxiv.org/abs/2412.04075)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geneb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
